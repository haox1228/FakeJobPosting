{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import ftfy\n",
    "from collections import Counter\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_id                 0.000000\n",
       "title                  0.000000\n",
       "location               0.019351\n",
       "department             0.645805\n",
       "salary_range           0.839597\n",
       "company_profile        0.185011\n",
       "description            0.000056\n",
       "requirements           0.150727\n",
       "benefits               0.403244\n",
       "telecommuting          0.000000\n",
       "has_company_logo       0.000000\n",
       "has_questions          0.000000\n",
       "employment_type        0.194128\n",
       "required_experience    0.394295\n",
       "required_education     0.453300\n",
       "industry               0.274217\n",
       "function               0.361018\n",
       "fraudulent             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the original dataset\n",
    "original = pd.read_csv(\"/Users/haox/Desktop/fake_job_postings.csv\")\n",
    "original.shape\n",
    "original.isnull().sum()/len(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to predict the fake job posting by classificationn model, we can drop the columns that does not deeply correlate with the prediction such as \"job_id\", \"title\", \"location\". Besides, since the dataset has a lot of missing or null value, we have to delete the columns that has a lot of missing value, such as \"department\"(64.58% null), \"salary_range\"(83.96% null), \"benefits\" (40.32% null), \"required_education (45.33% null)\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the remaining dataset consists \"telecommuting\", \"has_company_logo\", \"has_questionns\", \"description\", \"fradulent\". The column \"description\" has the detailed description of the job posting which would be the most helpful column for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the useful columns from the dataset.\n",
    "data = original.loc[:, [\"telecommuting\",\"has_company_logo\",\"has_questions\",\"description\",\"fraudulent\"]]\n",
    "# set the orders of the columns\n",
    "columns_order = [\"telecommuting\",\"has_company_logo\",\"has_questions\",\"description\",\"fraudulent\"]\n",
    "data = data[columns_order]\n",
    "\n",
    "# in the currrent dataset, there is still one row that has null value indescription, so we have to drop it.\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake job posting:  865\n",
      "real job posting:  17014\n"
     ]
    }
   ],
   "source": [
    "print(\"fake job posting: \", data.loc[data[\"fraudulent\"]==1, :].shape[0])\n",
    "print(\"real job posting: \", data.loc[data[\"fraudulent\"]==0, :].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (7,5))\n",
    "sns.countplot(x = data.fraudulent, data = data)\n",
    "plt.title('Fradulent vs Not Fraudulent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Fake job posting: 865\n",
    "*  Real job posting: 17014\n",
    "*  From the comparison of the fake and real job posting in this dataset, we know that there is a serious imbalance in the dataset. In order to balance the dataset for better utilization, we extract all the fake job posting and then random sample 1000 real job posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "random.seed(42)\n",
    "# extract 1000 rows from the dataset where fraudulent equals to 0 for future model training\n",
    "data_fraudulent_0_idx = random.choices(data.loc[data[\"fraudulent\"]==0].index, k=1000)\n",
    "data_fraudulent_0 = data.loc[data_fraudulent_0_idx, :]\n",
    "\n",
    "# extract all the rows where fraudulent is 1\n",
    "data_fraudulent_1 = data.loc[data[\"fraudulent\"]==1, :]\n",
    "\n",
    "# concatenate the dataframe above into a new dataframe\n",
    "data = pd.concat([data_fraudulent_0, data_fraudulent_1], axis=0)\n",
    "#Use shuffle in the sklearn.util to randomly reorder the rows in the new dataframe (we set the random seed as 42).\n",
    "data = shuffle(data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the text of \"description\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we use the module nltk to tokenize the \"description\" column to extract features to build NLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocess the text of \"description\"\n",
    "\n",
    "#in order to analyze the text in description, we first have to deal with the space, line feeds and url.\n",
    "# def preprocess(text):\n",
    "\n",
    "#     multispace_re = re.compile(r\"\\s{2,}\")\n",
    "#     newline_re = re.compile(r\"\\n+\")\n",
    "#     url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "#     # emoji_re = re.compile(\"([\\U00010000-\\U0010ffff]|[\\uD800-\\uDBFF][\\uDC00-\\uDFFF])\")\n",
    "\n",
    "#     preprocessed_text = multispace_re.sub(\"\",text)\n",
    "#     preprocessed_text = newline_re.sub(\"\",preprocessed_text)\n",
    "#     preprocessed_text = url_re.sub(\"url\", preprocessed_text)\n",
    "#     preprocessed_text = ftfy.fix_text(preprocessed_text)\n",
    "#     return preprocessed_text\n",
    "\n",
    "\n",
    "# 2. use the word tokenize tool in nltk to split the words in the text of description\n",
    "def nltk_word_tokenize(text, advanced_tokenize=True):\n",
    "    if advanced_tokenize:\n",
    "        advanced_text_tokenize = nltk.tokenize.TweetTokenizer()\n",
    "        return advanced_text_tokenize.tokenize(text)\n",
    "    else:\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "# 3. create a class to generate a document to restore all the words that were splitted in the 'description' column by using nltk module\n",
    "class Document():\n",
    "    def __init__(self):\n",
    "        # create a token list to store all the words after split\n",
    "        self.tokens_list = []\n",
    "        # initialize an int to count how many words after split\n",
    "        self.num_tokens = 0\n",
    "        # store the processed words\n",
    "        self.text = \"\"\n",
    "        \n",
    "\n",
    "    def extract_features_from_text(self,text):\n",
    "\n",
    "        # preprocessed_text = preprocess(text)\n",
    "        # self.text += preprocessed_text\n",
    "        self.text += text\n",
    "\n",
    "        # extract words\n",
    "        tokens = nltk_word_tokenize(text)\n",
    "        self.tokens_list.extend(tokens)\n",
    "        self.num_tokens += len(tokens)\n",
    " \n",
    "\n",
    "    def extract_features_from_texts(self, texts):\n",
    "        for text in texts:\n",
    "            self.extract_features_from_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part, in order to build the classification model, we first have to vectorize the data\n",
    "* Then we use three algorithm to train the model\n",
    "    * Logistic Regression\n",
    "    * SVM Machine\n",
    "    * Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the skip-gram model on gensim.models.word2vec to featurize the words and build the training model for the \"description\" column\n",
    "# The reason we choose the skip-gram model instead of CBOW model is because skip-gram is more sensitive to the low frequency words because of the nature of dense vectors.\n",
    "# reference source\n",
    "# https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# https://blog.csdn.net/zl_best/article/details/53433072?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import StratifiedKFold,cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "class Fake_job_cls_models():\n",
    "    def __init__(self,data_df):\n",
    "        self.data_df = data_df.copy()\n",
    "        # put the other columns except \"description\" into a new dataframe and turn the dataframe into a Numpy array for future use\n",
    "        self.data_other = np.array( self.data_df.loc[:,[\"telecommuting\",\"has_company_logo\",\"has_questions\"]] )\n",
    "        \n",
    "        # make a new dataframe that only store the \"description\" column so that we can do word embedding\n",
    "        self.data_description_df = self.data_df[\"description\"].to_frame()\n",
    "        # description_sen_tokens stores all the features of descrption after tokenized\n",
    "        self.description_sen_tokens = None\n",
    "        # initialize a variable to store the trained Skip-gram model in order to extra a particular feature vector\n",
    "        self.word2vec_model = self.description_word2vec_model(self.data_description_df)\n",
    "        \n",
    "        # store the density vector that were transferred from \"description\"\n",
    "        self.data_description_vectors = self.description_vectorized_data_creation(self.data_description_df, mean_vector = True)\n",
    "        \n",
    "        #stack horizontally the dense vector and data.other which are already in numpy array \n",
    "        self.X = np.hstack( (self.data_other, self.data_description_vectors) )\n",
    "        \n",
    "        # turn the fradulent column into numpy array\n",
    "        self.Y = np.array( self.data_df[\"fraudulent\"] )\n",
    "\n",
    "    # from the last part, we build a class to tokenize all the text descriotion. \n",
    "    # Create Document object to store the tokenized data.\n",
    "    def corpus_creation(self, data_description_df):\n",
    "        for row in range(data_description_df.shape[0]):\n",
    "            doc = Document()\n",
    "            doc.extract_features_from_text(data_description_df.iloc[row,0])\n",
    "            yield doc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def description_word2vec_model(self, data_description_df):\n",
    "        # create a corpus list to store all the tokenized documents object in description\n",
    "        corpus = []\n",
    "        corpus.extend( self.corpus_creation(data_description_df) )\n",
    "        # description_sen_tokens stores all the features of descrption after tokenized\n",
    "        self.description_sen_tokens = [doc.tokens_list for doc in corpus]\n",
    "\n",
    "        # The model is based on the Skip-gram model in word2vec (set sg = 1). We use this model to vectorize the all the features\n",
    "        # of the tokenized words into word embedding with a size of 300.\n",
    "        # The reason we use Skip-gram instead of CBOW is because for a detailed descrpition like our data, Skip-gram will better capture the semantic between words\n",
    "        # We use negative sampling (hs = 0) to improve the performance.\n",
    "        # we set negative = 10 which means for each of every words we will have 10 negative sample\n",
    "        # by capturing these negative samples, we would reduce the size of matrix significantly\n",
    "        word2vec_model = word2vec.Word2Vec(sentences=self.description_sen_tokens, size=300, window=9, min_count=1, sg=1, hs=0,\n",
    "                                           negative=10, seed=42, iter=5, workers=4)\n",
    "        return word2vec_model\n",
    "    \n",
    "    \n",
    "\n",
    "    # The reason we use word embedding is because the vector could represent the density of the tokenized word in the text.\n",
    "    # We use the mean (mean_vector = true) of the word embedding to represent the density of the pattern.\n",
    "    # comparing to the \"bag of words\" method which would give us sparse feature pattern, we could get a dense vector\n",
    "    # where most of the value is non-zero.\n",
    "    # We could finish the NLP step by vectorizing the natural languae using after it finish all the iteration and completely insert all the density values\n",
    "    def description_vectorized_data_creation(self, data_description_df, mean_vector = True):\n",
    "        \n",
    "        # initialize an empty numpy arrary to store the density of the vector, the column the number of tokenized word and the row is the size of the word embedding\n",
    "        data_description_vectors = np.zeros( (len(self.description_sen_tokens), 300) )\n",
    "\n",
    "        for num in range(len(self.description_sen_tokens)):\n",
    "            each_description_tokens = self.description_sen_tokens[num]\n",
    "            each_description_vectors = None\n",
    "            \n",
    "            # if the length of the description token is not zero\n",
    "            if len(each_description_tokens) != 0:\n",
    "                \n",
    "                if mean_vector:\n",
    "                    each_description_vectors = np.vstack((self.word2vec_model.wv[token] for token in each_description_tokens))\n",
    "                    # get the density by getting the mean value of the 300-size word embedding vector\n",
    "                    each_description_vectors = np.mean(each_description_vectors, axis=0)\n",
    "\n",
    "                else:\n",
    "                    each_description_vectors = np.vstack((self.word2vec_model.wv[token] for token in each_description_tokens))\n",
    "                    # if the mean_vector is false, get the density by getting the sum value of the word embedding\n",
    "                    each_description_vectors = np.sum(each_description_vectors, axis=0)\n",
    "\n",
    "                # after each of the iteration, update the data description vector by inserting the density of the word vector \n",
    "                data_description_vectors[num] = each_description_vectors\n",
    "                \n",
    "            else:\n",
    "            # if the length of the description token is zero, print the number of them\n",
    "                print(\"0 num:\",num)\n",
    "                continue\n",
    "        return data_description_vectors\n",
    "\n",
    "    \n",
    "    \n",
    "    # we build this method to see the important measure of evaluating the following classification model, such as the mean, standard deviation, min and max of\n",
    "    # accuracy, precision, recallï¼Œand F1\n",
    "    def print_cv_scores_summary(self, cv_scores):\n",
    "        def cv_score_summary(name,scores):\n",
    "            print(\"{}:  mean = {:.2f}%,  sd = {:.2f}%,  min = {:.2f}%,  max = {:.2f}%\".format(name, scores.mean() * 100,\n",
    "                                                                                        scores.std() * 100,\n",
    "                                                                                        scores.min() * 100,\n",
    "                                                                                        scores.max() * 100))\n",
    "\n",
    "        cv_score_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "        cv_score_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "        cv_score_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "        cv_score_summary(\"F1\", cv_scores['test_f1_weighted'])\n",
    "        \n",
    "    \n",
    "    # Here we use logistic regression. \n",
    "    # Since the essense of logistic regression is a linear regression with the sigmoid function which will also show the direction of association,\n",
    "    # which is suitable for classification model.\n",
    "    # We generate a 10 time cross validation and output the important measures that we mentioned above to evaluate the performance\n",
    "    def Logis_Regression(self):\n",
    "        time_start = time.time()\n",
    "        \n",
    "        logis_reg = LogisticRegression(penalty='l1',solver=\"liblinear\",max_iter=300)\n",
    "        logis_reg_cv_scores = cross_validate(logis_reg, self.X, self.Y, cv=StratifiedKFold(n_splits=10,random_state=0), return_train_score=False,\n",
    "                                       scoring=[\"accuracy\",\"precision_weighted\",\"recall_weighted\",\"f1_weighted\"])\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print(\"model run time: \", time_end - time_start, \"seconds\")\n",
    "        \n",
    "        print(\"After 10 times of cross validation of logistic regression, the results of measures are: \")\n",
    "        self.print_cv_scores_summary(logis_reg_cv_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # SVM\n",
    "    def SVM(self):\n",
    "        time_start = time.time()\n",
    "        \n",
    "        svm = NuSVC()\n",
    "        \n",
    "        # 10 times cross validation\n",
    "        svm_cv_scores = cross_validate(svm, self.X, self.Y, cv=StratifiedKFold(n_splits=10,random_state=0), return_train_score=False,\n",
    "                                       scoring=[\"accuracy\",\"precision_weighted\",\"recall_weighted\",\"f1_weighted\"])\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print(\"model run time: \", time_end - time_start, \"seconds\")\n",
    "        \n",
    "        print(\"After 10 times of cross validation of SVM, the results of measures are: \")\n",
    "        self.print_cv_scores_summary(svm_cv_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # FNN\n",
    "    def FNN(self):\n",
    "        time_start = time.time()\n",
    "        \n",
    "        # Build the neural network, the first hidder layer is 30, the second is 20\n",
    "        nn = MLPClassifier(hidden_layer_sizes=(30,20), activation='relu', learning_rate=\"adaptive\", max_iter=500,\n",
    "                          warm_start=True)\n",
    "        \n",
    "        # 10 times cross validation\n",
    "        nn_cv_scores = cross_validate(nn, self.X, self.Y, cv=StratifiedKFold(n_splits=10,random_state=0), return_train_score=False,\n",
    "                                       scoring=[\"accuracy\",\"precision_weighted\",\"recall_weighted\",\"f1_weighted\"])\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print(\"model run time: \", time_end - time_start, \"seconds\")\n",
    "        \n",
    "        print(\"After 10 times of cross validation of FNN, the results of measures are:: \")\n",
    "        self.print_cv_scores_summary(nn_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 num: 225\n"
     ]
    }
   ],
   "source": [
    "fake_job_cls_models = Fake_job_cls_models(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words or phrase that most similar to 'experience' is:\n",
      "[('years', 0.7516058683395386), ('programming', 0.6670236587524414), ('preferably', 0.6629071235656738), ('Minimum', 0.6620178818702698), ('Experience', 0.6596102714538574), ('10', 0.6489766836166382), ('MBA', 0.6471438407897949), ('5', 0.6471325159072876), ('demonstrable', 0.6455799341201782), ('qualifications', 0.6406989097595215)]\n",
      "====================================================================================================\n",
      "The words or phrase that most similar to 'part-time' is:\n",
      "[('full-time', 0.9222885370254517), ('dual-licensed', 0.8181516528129578), ('fields', 0.7893139123916626), ('career', 0.7832638025283813), ('variety', 0.7765883207321167), ('homes', 0.7719781994819641), ('California', 0.7713143825531006), ('find', 0.7678960561752319), ('hourly', 0.7671976089477539), ('schedules.In', 0.763117790222168)]\n"
     ]
    }
   ],
   "source": [
    "print(\"The words or phrase that most similar to 'experience' is:\")\n",
    "print(fake_job_cls_models.word2vec_model.wv.most_similar(\"experience\"))\n",
    "print(\"=\"*100)\n",
    "print(\"The words or phrase that most similar to 'part-time' is:\")\n",
    "print(fake_job_cls_models.word2vec_model.wv.most_similar(\"part-time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model run time:  2.192249059677124 seconds\n",
      "After 10 times of cross validation of logistic regression, the results of measures are: \n",
      "Accuracy:  mean = 81.39%,  sd = 2.39%,  min = 77.96%,  max = 86.10%\n",
      "Precision:  mean = 81.49%,  sd = 2.34%,  min = 78.44%,  max = 86.18%\n",
      "Recall:  mean = 81.39%,  sd = 2.39%,  min = 77.96%,  max = 86.10%\n",
      "F1:  mean = 81.34%,  sd = 2.43%,  min = 77.67%,  max = 86.05%\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model\n",
    "fake_job_cls_models.Logis_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model run time:  10.353416204452515 seconds\n",
      "After 10 times of cross validation of SVM, the results of measures are: \n",
      "Accuracy:  mean = 79.67%,  sd = 3.07%,  min = 75.81%,  max = 85.03%\n",
      "Precision:  mean = 79.84%,  sd = 2.94%,  min = 76.57%,  max = 85.05%\n",
      "Recall:  mean = 79.67%,  sd = 3.07%,  min = 75.81%,  max = 85.03%\n",
      "F1:  mean = 79.60%,  sd = 3.13%,  min = 75.35%,  max = 85.00%\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "fake_job_cls_models.SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model run time:  25.881951808929443 seconds\n",
      "After 10 times of cross validation of FNN, the results of measures are:: \n",
      "Accuracy:  mean = 87.24%,  sd = 2.12%,  min = 81.82%,  max = 89.25%\n",
      "Precision:  mean = 87.34%,  sd = 2.00%,  min = 82.32%,  max = 89.28%\n",
      "Recall:  mean = 87.24%,  sd = 2.12%,  min = 81.82%,  max = 89.25%\n",
      "F1:  mean = 87.23%,  sd = 2.13%,  min = 81.83%,  max = 89.25%\n"
     ]
    }
   ],
   "source": [
    "# FNN model\n",
    "fake_job_cls_models.FNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
